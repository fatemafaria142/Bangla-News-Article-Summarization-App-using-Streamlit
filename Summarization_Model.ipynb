{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemafaria142/Bangla-News-Article-Summarization-App-using-Streamlit/blob/main/Summarization_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfCjVtiVhUX9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tO2EyQHeTir"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxaRGN6dObR2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3W0FgQxQAKV"
      },
      "source": [
        "# **Loading datastet from huggingface**\n",
        "* Dataset link: https://huggingface.co/datasets/csebuetnlp/xlsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w5yGcoAOD3G"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csebuetnlp/xlsum\",'bengali')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bpbkpt7O0Gm"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koSKDC1KO0Nw"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({\n",
        "    'text': dataset['train']['text'],\n",
        "    'summary':dataset['train']['summary']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Eda3BAxP61v"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP_LYL61a7gT"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDgr_BGgfuzR"
      },
      "outputs": [],
      "source": [
        "def preprocess_bangla_text(text):\n",
        "    # Remove website links\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove email links\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove specified punctuation\n",
        "    punctuation = r'!#$%&\\()*+-./:;?@[\\]^_`{|}~'\n",
        "    text = re.sub('[' + re.escape(punctuation) + ']', '', text)\n",
        "\n",
        "    # Remove extra white spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply preprocessing to 'text' and 'summary' columns\n",
        "data['text_preprocessed'] = data['text'].apply(preprocess_bangla_text)\n",
        "data['summary_preprocessed'] = data['summary'].apply(preprocess_bangla_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgPqvHmJe-Dk"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMaVyMaHgTGY"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['text','summary'],axis=1)\n",
        "\n",
        "# Rename columns 'text_preprocessed' and 'summary_preprocessed' to 'text' and 'summary'\n",
        "data = data.rename(columns={'text_preprocessed': 'text', 'summary_preprocessed': 'summary'})\n",
        "\n",
        "# Display the DataFrame after renaming columns\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcC13RasRVzg"
      },
      "source": [
        "**From main train dataset, I've taken 3000 samples for train, 500 samples for test, and 500 samples for validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuAei1dBQUUQ"
      },
      "outputs": [],
      "source": [
        "train = data[:4000]\n",
        "test = data[4000:4500]\n",
        "validation = data[4500:5000]\n",
        "\n",
        "train.to_csv('/content/drive/MyDrive/summary_train.csv',index=False)\n",
        "test.to_csv('/content/drive/MyDrive/summary_test.csv',index=False)\n",
        "validation.to_csv('/content/drive/MyDrive/summary_validation.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_S-3Q4Hxz3J"
      },
      "source": [
        "# **Loading Train Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPSp3aJzSQBZ"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/summary_train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9wpuogAcOM_"
      },
      "source": [
        "# **Checking NaN values in Train dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGu_GglJRsIy"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in df_train\n",
        "nan_count = df_train.isna().sum()\n",
        "\n",
        "# Display columns with NaN values and their respective counts\n",
        "print(\"Columns with NaN values:\")\n",
        "print(nan_count[nan_count > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzzgw9-ITLLZ"
      },
      "source": [
        "# **Loading Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFKWx6KNQJ_a"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/summary_test.csv')\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exj8Qi01cZK8"
      },
      "source": [
        "# **Checking NaN values in Test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dUlIXuoR-9J"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in df_train\n",
        "nan_count = df_test.isna().sum()\n",
        "\n",
        "# Display columns with NaN values and their respective counts\n",
        "print(\"Columns with NaN values:\")\n",
        "print(nan_count[nan_count > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE31936cXaGn"
      },
      "source": [
        "# **Count of unique words for text and summary in Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mg-vp5CXWDA"
      },
      "outputs": [],
      "source": [
        "# Tokenize 'text' and 'passage' columns to count unique words\n",
        "text_words = ' '.join(df_test['text']).split()\n",
        "summary_words = ' '.join(df_test['summary']).split()\n",
        "\n",
        "# Calculate unique words\n",
        "unique_text_words = len(set(text_words))\n",
        "unique_summary_words = len(set(summary_words))\n",
        "\n",
        "# Display the count of unique words\n",
        "print(f\"Number of unique words in 'text': {unique_text_words}\")\n",
        "print(f\"Number of unique words in 'summary': {unique_summary_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mSiI0gMTOvh"
      },
      "source": [
        "# **Validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDpM59b6QKZa"
      },
      "outputs": [],
      "source": [
        "df_validation = pd.read_csv('/content/drive/MyDrive/summary_validation.csv')\n",
        "df_validation.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vALPuSj4ccyd"
      },
      "source": [
        "# **Checking NaN values in validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6pmGPjTSC6z"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in df_train\n",
        "nan_count = df_validation.isna().sum()\n",
        "\n",
        "# Display columns with NaN values and their respective counts\n",
        "print(\"Columns with NaN values:\")\n",
        "print(nan_count[nan_count > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qrWj3GEyjfE"
      },
      "source": [
        "# **Checking length of the datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJIdlrGzanhf"
      },
      "outputs": [],
      "source": [
        "print(\"Train Dataset Length: \",len(df_train))\n",
        "print(\"Test Dataset Length: \",len(df_test))\n",
        "print(\"Validation Dataset Length: \",len(df_validation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykparBRpYa_6"
      },
      "source": [
        "# **Installing Necesary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO_XH6OCjtS9"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/csebuetnlp/normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K1LEadElTu7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T83lX2EOlt6D"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWXgaZdbrs3m"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DChfPXAArkmE"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E6JdffvrwWy"
      },
      "outputs": [],
      "source": [
        "!transformers-cli cache clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rE0pDj1ry17"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.10.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7X8wHv0ry5b"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate==0.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wQBCxEhr1fb"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDuWETyk8gJ2"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxC2P3VU-Gnk"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWAQpf-6EEOI"
      },
      "source": [
        "# **Apply normalization to the datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiuUBx4gECAp"
      },
      "outputs": [],
      "source": [
        "from normalizer import normalize\n",
        "# Apply normalization to the datasets\n",
        "df_train['text'] = df_train['text'].apply(normalize)\n",
        "df_train['summary'] = df_train['summary'].apply(normalize)\n",
        "\n",
        "df_test['text'] = df_test['text'].apply(normalize)\n",
        "df_test['summary'] = df_test['summary'].apply(normalize)\n",
        "\n",
        "df_validation['text'] = df_validation['text'].apply(normalize)\n",
        "df_validation['summary'] = df_validation['summary'].apply(normalize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnvyBJQmkjaV"
      },
      "source": [
        "# **BanglaT5 model and Its Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24UaId2ciENU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from normalizer import normalize # pip install git+https://github.com/csebuetnlp/normalizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\", use_fast=True) #sentencepiece library is required to instantiate the fast tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIYic5-DoDJn"
      },
      "source": [
        "# **Custom dataset class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAkUy3qjn8Hk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class BanglaSummaryDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=520):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['text']\n",
        "        summary = self.data.iloc[idx]['summary']\n",
        "\n",
        "\n",
        "        # Tokenize text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize summary\n",
        "        tokenized_summary = self.tokenizer(\n",
        "            summary,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Process tokenized answer labels\n",
        "        labels = tokenized_summary['input_ids'].squeeze()\n",
        "        attention_mask_labels = tokenized_summary['attention_mask'].squeeze()\n",
        "\n",
        "        # Ignore padded tokens during training\n",
        "        labels[attention_mask_labels == 0] = -100\n",
        "\n",
        "        # Handle cases where the input sequence is longer than max_length\n",
        "        if inputs['input_ids'].shape[1] > self.max_length:\n",
        "            inputs['input_ids'] = inputs['input_ids'][:, :self.max_length]\n",
        "            inputs['attention_mask'] = inputs['attention_mask'][:, :self.max_length]\n",
        "            attention_mask_labels = attention_mask_labels[:self.max_length]\n",
        "\n",
        "        # Ensuring labels and attention_mask_labels have the same length\n",
        "        labels = labels[:self.max_length]\n",
        "\n",
        "        # Using .squeeze() on labels\n",
        "        labels = labels.squeeze()\n",
        "\n",
        "        # Return as dictionaries\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels,\n",
        "\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wElCiK8sm9U"
      },
      "source": [
        "# **Create datasets and data loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb8kL4fEr2_i"
      },
      "outputs": [],
      "source": [
        "train_dataset = BanglaSummaryDataset(df_train, tokenizer)\n",
        "validation_dataset = BanglaSummaryDataset(df_validation, tokenizer)\n",
        "test_dataset = BanglaSummaryDataset(df_test, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset , batch_size=10, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset , batch_size=10, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset , batch_size=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcUj0N1PtV5A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQJfrWUCHZk"
      },
      "source": [
        "# **Training Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s05ZdN7jQWjd"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# Create a custom optimizer using torch.optim.AdamW\n",
        "custom_optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,  # Learning rate\n",
        "    eps=1e-8,  # Epsilon value to prevent division by zero\n",
        "    weight_decay=0.01,  # Weight decay (L2 regularization)\n",
        ")\n",
        "\n",
        "# Define the TrainingArguments for question answering\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients over 8 small batches\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        "    save_steps=1000,\n",
        "    learning_rate=1e-3,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    lr_scheduler_type=\"cosine_with_restarts\",  # Setting the learning rate scheduler type\n",
        "    warmup_steps=100,  # Number of warmup steps\n",
        "    weight_decay=0.01,  # Weight decay (L2 regularization)\n",
        "    logging_dir='/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary',  # Using the same directory for logs\n",
        "    logging_steps=500,  # Log every 500 steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAB7Zx0ECNd0"
      },
      "source": [
        "# **Custom Data Collator for Summary Generation**\n",
        "* A custom data collator for summary generation is used to handle the specific data formatting required for training models on question answering tasks. It facilitates the processing of input data into a format suitable for model training by performing tasks such as padding, truncation, and organizing inputs and labels for the Summary model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPN_-QElRGaN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
        "    def __call__(self, features):\n",
        "        batch = super().__call__(features)\n",
        "\n",
        "        # Find the maximum length of labels in the batch\n",
        "        max_label_length = max(feature[\"labels\"].shape[-1] for feature in features)\n",
        "\n",
        "        # Pad or truncate the \"labels\" arrays to have the same length\n",
        "        batch[\"labels\"] = torch.stack([\n",
        "            F.pad(torch.as_tensor(feature[\"labels\"]), (0, max_label_length - feature[\"labels\"].shape[-1]), value=-100)\n",
        "            for feature in features\n",
        "        ])\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Creating a data collator using the custom collator\n",
        "data_collator = CustomDataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    label_pad_token_id=tokenizer.pad_token_id,\n",
        "    pad_to_multiple_of=8  # Ensuring the sequence length is a multiple of 8 (adjust as needed)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfj79KgmCYgZ"
      },
      "source": [
        "# **Trainer**\n",
        "* The trainer package provides utilities to write re-usable training scripts. The core idea is to use a trainer that implements a nested loop, where the outer loop runs the data collection steps and the inner loop the optimization steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulNKjX01B2dx"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "# Define the Trainer with the custom optimizer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    optimizers=(custom_optimizer, None),  # Passing the custom optimizer here\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDgD7RhsCuK8"
      },
      "source": [
        "# **Training Starts Here**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q484smgo2jAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "Wb_tTHYCjnSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"Soyeda10/BanglaTextSummarization\")"
      ],
      "metadata": {
        "id": "n44ZECfNkRyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdZyvt5cEy6a"
      },
      "source": [
        "# **Saving model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/Bangla_Summary_BanglaT5_Model.pt')\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/Bangla_Summary_BanglaT5_Tokenizer.json')\n"
      ],
      "metadata": {
        "id": "2P3-cuRkOnsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1krjJoQnE7Et"
      },
      "source": [
        "# **Loading trained model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE1N8RUXE4xW"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "# Load the saved model\n",
        "model = MT5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/Bangla_Summary_BanglaT5_Model.pt')\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/Bangla_Summary_BanglaT5_Tokenizer.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ltoIS1TjkUN"
      },
      "source": [
        "# **Evaluation Metrics Installing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDZ0V2x4jTnN"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Fsi-fSjv8Q"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AssfmZs_jwDZ"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDUhtuAo4xwy"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zdztQmJ55iF"
      },
      "source": [
        "# **Testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_predictions_batch = []\n",
        "decoded_references_batch = []\n",
        "\n",
        "# Loop through the test dataset and generate predictions\n",
        "for batch in test_dataloader:\n",
        "    inputs = batch['input_ids'].to('cuda')  # Move inputs to the 'cuda' device\n",
        "    labels = batch['labels'].to('cuda')  # Move labels to the 'cuda' device\n",
        "\n",
        "    # Ensuring the model, inputs, and labels are on the same device\n",
        "    model = model.to(inputs.device)\n",
        "\n",
        "    # Generating answers using our model\n",
        "    with torch.no_grad():\n",
        "        # Generating predictions\n",
        "        predictions = model.generate(inputs, max_length=520, num_beams=4, early_stopping=True)\n",
        "\n",
        "        # Decoding generated answers\n",
        "        try:\n",
        "            decoded_predictions_batch.extend(tokenizer.batch_decode(predictions.cpu(), skip_special_tokens=True))\n",
        "        except Exception as e:\n",
        "            print(\"Exception occurred during decoding (predictions):\", e)\n",
        "\n",
        "\n",
        "        # Decoding labels for references\n",
        "        try:\n",
        "            # Converting labels to a list of token IDs\n",
        "            labels_list = labels.cpu()\n",
        "             # Decoding each label sequence\n",
        "            decoded_references_batch.extend([tokenizer.decode(ids.clamp(0, tokenizer.vocab_size - 1), skip_special_tokens=True) for ids in labels_list])\n",
        "        except Exception as e:\n",
        "            print(\"Exception occurred during decoding (references):\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "9nsg6MZT0C3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAsX3psxG5Aa"
      },
      "outputs": [],
      "source": [
        "print(decoded_predictions_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JmnHgC7dW1O"
      },
      "outputs": [],
      "source": [
        "print(len(decoded_predictions_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8b0fmLPG9ir"
      },
      "outputs": [],
      "source": [
        "print( decoded_references_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpLT5DsFdajG"
      },
      "outputs": [],
      "source": [
        "print(len(decoded_references_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuFG3eZW81Qk"
      },
      "source": [
        "# **Metrics Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhQngY30ny7K"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "# Loading the evaluation metric for Character Error Rate (CER) and Word Error Rate (WER) and Exact Match(em)\n",
        "cer_metric = load(\"cer\")\n",
        "wer_metric = load(\"wer\")\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "\n",
        "# Loading BLEU and METEOR metrics\n",
        "bleu_metric = load(\"bleu\")\n",
        "meteor = load('meteor')\n",
        "\n",
        "# Calculating Character Error Rate (CER), Word Error Rate (WER) and Exact Match (EM)\n",
        "results_CER = cer_metric.compute(predictions=decoded_predictions_batch, references=decoded_references_batch)\n",
        "results_WER = wer_metric.compute(predictions=decoded_predictions_batch, references=decoded_references_batch)\n",
        "results_em = exact_match_metric.compute(predictions=decoded_predictions_batch, references=decoded_references_batch)\n",
        "\n",
        "# Calculating Bilingual Evaluation Understudy (BLEU) , Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and METEOR(M)\n",
        "results_bleu = bleu_metric.compute(predictions=decoded_predictions_batch, references=decoded_references_batch)\n",
        "results_met = meteor.compute(predictions=decoded_predictions_batch, references=decoded_references_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoPok_hinuxQ"
      },
      "outputs": [],
      "source": [
        "print(\"Character Error Rate for Bangla Summary:\", results_CER)\n",
        "print(\"Word Error Rate for Bangla Summary:\",results_WER)\n",
        "print(\"Exact Match for Bangla Summary:\",results_em)\n",
        "print(\"BLEU Score for Bangla Summary:\",results_bleu)\n",
        "print(\"METEOR for Bangla Summary:\",results_met)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bBtYthf2pPn"
      },
      "outputs": [],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCW2KCx42qmW"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROUGE Score Calculation**"
      ],
      "metadata": {
        "id": "ddivZThAAz0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHtHMnvM2xJ5"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Initialize the Rouge scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = unidecode(text)\n",
        "    tokens = text.split()\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Calculate scores for each pair of predictions and references\n",
        "rouge1_f1_scores = []\n",
        "rouge1_precision_scores = []\n",
        "rouge1_recall_scores = []\n",
        "rouge2_f1_scores = []\n",
        "rouge2_precision_scores = []\n",
        "rouge2_recall_scores = []\n",
        "rougeL_f1_scores = []\n",
        "rougeL_precision_scores = []\n",
        "rougeL_recall_scores = []\n",
        "\n",
        "for ref, pred in zip(decoded_references_batch, decoded_predictions_batch):\n",
        "    candidate_summary = preprocess_text(pred)\n",
        "    reference_summary = preprocess_text(' '.join(ref))\n",
        "    scores = scorer.score(reference_summary, candidate_summary)\n",
        "\n",
        "    rouge1_f1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge1_precision_scores.append(scores['rouge1'].precision)\n",
        "    rouge1_recall_scores.append(scores['rouge1'].recall)\n",
        "    rouge2_f1_scores.append(scores['rouge2'].fmeasure)\n",
        "    rouge2_precision_scores.append(scores['rouge2'].precision)\n",
        "    rouge2_recall_scores.append(scores['rouge2'].recall)\n",
        "    rougeL_f1_scores.append(scores['rougeL'].fmeasure)\n",
        "    rougeL_precision_scores.append(scores['rougeL'].precision)\n",
        "    rougeL_recall_scores.append(scores['rougeL'].recall)\n",
        "\n",
        "# Calculate the average scores\n",
        "avg_rouge1_f1 = sum(rouge1_f1_scores) / len(rouge1_f1_scores)\n",
        "avg_rouge1_precision = sum(rouge1_precision_scores) / len(rouge1_precision_scores)\n",
        "avg_rouge1_recall = sum(rouge1_recall_scores) / len(rouge1_recall_scores)\n",
        "avg_rouge2_f1 = sum(rouge2_f1_scores) / len(rouge2_f1_scores)\n",
        "avg_rouge2_precision = sum(rouge2_precision_scores) / len(rouge2_precision_scores)\n",
        "avg_rouge2_recall = sum(rouge2_recall_scores) / len(rouge2_recall_scores)\n",
        "avg_rougeL_f1 = sum(rougeL_f1_scores) / len(rougeL_f1_scores)\n",
        "avg_rougeL_precision = sum(rougeL_precision_scores) / len(rougeL_precision_scores)\n",
        "avg_rougeL_recall = sum(rougeL_recall_scores) / len(rougeL_recall_scores)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"Average Rouge-1 F1 Score:\", avg_rouge1_f1)\n",
        "print(\"Average Rouge-1 Precision:\", avg_rouge1_precision)\n",
        "print(\"Average Rouge-1 Recall:\", avg_rouge1_recall)\n",
        "\n",
        "print(\"Average Rouge-2 F1 Score:\", avg_rouge2_f1)\n",
        "print(\"Average Rouge-2 Precision:\", avg_rouge2_precision)\n",
        "print(\"Average Rouge-2 Recall:\", avg_rouge2_recall)\n",
        "\n",
        "print(\"Average Rouge-L F1 Score:\", avg_rougeL_f1)\n",
        "print(\"Average Rouge-L Precision:\", avg_rougeL_precision)\n",
        "print(\"Average Rouge-L Recall:\", avg_rougeL_recall)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving results to a csv file**"
      ],
      "metadata": {
        "id": "iKyJoHBcBGz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZcCal7TI4xq"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame to store translations\n",
        "generated_summary_df = pd.DataFrame({\n",
        "    'text': df_test['text'],  # Assuming 'test_data' contains our test dataset\n",
        "    'Reference_Summary': decoded_references_batch,\n",
        "    'Generated_Summary': decoded_predictions_batch\n",
        "})\n",
        "\n",
        "# Save translations to a CSV file\n",
        "generated_summary_df.to_csv(\"/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/BanglaT5_generated_answers.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Information Lost Calculation**"
      ],
      "metadata": {
        "id": "igBloYatAweP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_information_lost(original_summary, generated_summary):\n",
        "    # Tokenize the text into unique tokens\n",
        "    original_tokens = set(original_summary.split())\n",
        "    generated_tokens = set(generated_summary.split())\n",
        "\n",
        "    # Calculate the number of unique tokens before and after processing\n",
        "    num_original_tokens = len(original_tokens)\n",
        "    num_generated_tokens = len(generated_tokens)\n",
        "\n",
        "    # Calculate Word Information Lost (WIL)\n",
        "    wil = 1 - (num_generated_tokens / num_original_tokens) if num_original_tokens != 0 else 0\n",
        "\n",
        "    return wil"
      ],
      "metadata": {
        "id": "4y382ropAvS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIL_df = pd.read_csv(\"/content/drive/MyDrive/Bangla_Summary/BanglaT5_Bangla_Summary/BanglaT5_generated_answers.csv\")\n",
        "WIL_df.head()"
      ],
      "metadata": {
        "id": "Kt6HYVdYCSFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function row-wise using apply along with axis=1\n",
        "WIL_df['Word_Information_Lost'] = WIL_df.apply(lambda row: calculate_word_information_lost(row['Reference_Summary'], row['Generated_Summary']), axis=1)\n",
        "\n",
        "# Display the resulting DataFrame with the Word Information Lost column\n",
        "WIL_df.head()\n"
      ],
      "metadata": {
        "id": "FKgAsNMwCXeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean Word Information Lost across all rows\n",
        "mean_wil = WIL_df['Word_Information_Lost'].mean()\n",
        "print(f\"Mean Word Information Lost (WIL): {mean_wil:.2f}\")"
      ],
      "metadata": {
        "id": "-IlsUr_rDRYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}